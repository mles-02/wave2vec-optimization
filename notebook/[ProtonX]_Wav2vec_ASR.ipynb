{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukncmUREYjT5"
      },
      "source": [
        "# Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9jyjDbhtLRw"
      },
      "outputs": [],
      "source": [
        "BASE_WAV2VEC_MODEL = \"nguyenvulebinh/wav2vec2-base-vietnamese-250h\"\n",
        "BASE_WAV2VEC_PROCESSOR = BASE_WAV2VEC_MODEL\n",
        "OUTPUT_DIR= 'D:\\Coding\\wave2vec-optimization\\data\\data\\data'\n",
        "OUTPUT_DIR_AUDIO = 'D:\\Coding\\wave2vec-optimization\\data\\data\\data\\\\audio'\n",
        "# DATA_DIR = [\"/content/drive/MyDrive/Colab Notebooks/labeled-audio\"]\n",
        "MY_MODEL_DIR = \"D:\\Coding\\wave2vec-optimization\\data\\checkpoints\\checkpoints\"\n",
        "\n",
        "# BASE_WAV2VEC_MODEL = \"nguyenvulebinh/wav2vec2-base-vietnamese-250h\"\n",
        "# BASE_WAV2VEC_PROCESSOR = BASE_WAV2VEC_MODEL\n",
        "# OUTPUT_DIR= '/content/drive/MyDrive/data'\n",
        "# OUTPUT_DIR_AUDIO = '/content/drive/MyDrive/data/audio'\n",
        "# # DATA_DIR = [\"/content/drive/MyDrive/Colab Notebooks/labeled-audio\"]\n",
        "# MY_MODEL_DIR = \"/content/drive/MyDrive/Colab Notebooks/checkpoints\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCwzEsYtYvcv"
      },
      "source": [
        "# Data preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH5T8hhDzV-F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import uuid\n",
        "\n",
        "def get_audio_file(folder, txt_file):\n",
        "  exts = ['wav', 'm4a', 'mp3']\n",
        "\n",
        "  name = os.path.basename(txt_file)[:-4]\n",
        "\n",
        "  for ext in exts:\n",
        "    audio_files = glob.glob(glob.escape(folder) + \"/**/\" + name + '.' + ext, recursive=True)\n",
        "    if len(audio_files) > 0:\n",
        "      return audio_files[0]\n",
        "\n",
        "  raise Exception(f\"audio file not found: {name}\")\n",
        "\n",
        "def split_audio(txt_file, audio_file, output_dir):\n",
        "  if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "  # Get audio file\n",
        "  audio_data, sample_rate = librosa.load(audio_file, sr=16000)\n",
        "\n",
        "  results = []\n",
        "  with open(txt_file) as file:\n",
        "    for line in tqdm(file):\n",
        "      data = line.strip().split('\\t')\n",
        "      if len(data) != 3:\n",
        "        continue\n",
        "\n",
        "      start = int(float(data[0]) * sample_rate)\n",
        "      stop = int(float(data[1]) * sample_rate)\n",
        "      if stop - start > 10 * sample_rate:\n",
        "        continue\n",
        "\n",
        "      text = data[2]\n",
        "\n",
        "      audio_part = audio_data[start:stop]\n",
        "      audio_part = np.concatenate([np.zeros((int(0.5 * sample_rate),)), audio_part, np.zeros((int(0.5 * sample_rate),))]) # padding zero\n",
        "\n",
        "      audio_path = os.path.join(output_dir, str(uuid.uuid4()) + \".wav\")\n",
        "\n",
        "      with open(audio_path, 'wb') as out_file:\n",
        "        sf.write(out_file, audio_part, sample_rate)\n",
        "\n",
        "      results.append({\n",
        "        \"file\": audio_path,\n",
        "        \"text\": text\n",
        "      })\n",
        "  return results\n",
        "\n",
        "def split_folder(folder):\n",
        "  print(f\"Split folder {folder}\")\n",
        "\n",
        "  results = []\n",
        "  # Get list file txt\n",
        "  txt_files = glob.glob(glob.escape(folder) + \"/**/*.txt\", recursive=True)\n",
        "  for txt_file in txt_files:\n",
        "    audio_file = get_audio_file(folder, txt_file)\n",
        "    print(\"Subtitle file: \", txt_file)\n",
        "    print(\"Audio file: \", audio_file)\n",
        "    print()\n",
        "\n",
        "    result = split_audio(txt_file, audio_file, OUTPUT_DIR_AUDIO)\n",
        "    results.extend(result)\n",
        "\n",
        "    dest_txt = os.path.join('data/raw_data', os.path.basename(txt_file))\n",
        "    os.system(f'cp \"{txt_file}\" \"{dest_txt}\"')\n",
        "    dest_audio = os.path.join('data/raw_data', os.path.basename(audio_file))\n",
        "    os.system(f'cp \"{audio_file}\" \"{dest_audio}\"')\n",
        "\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzDzGnsOzF2V"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "vocab = 'ẻ6ụí3ỹýẩởềõ7êứỏvỷalựqờjốàỗnéủуôuyằ4wbệễsìầỵ8dểrũcạ9ếùỡ2tiǵử̀á0ậeộmẳợĩhâúọồặfữắỳxóãổị̣zảđèừòẵ1ơkẫpấẽỉớẹăoư5|'\n",
        "def clear_text(row):\n",
        "  correct = [\n",
        "    ['\\\\Delta ', 'delta '],\n",
        "    ['\\\\arctan ', 'arctan '],\n",
        "    ['\\\\lim ', 'lim '],\n",
        "    ['\\\\ln ', 'ln '],\n",
        "    ['\\\\ln(x) ', 'ln x '],\n",
        "    ['\\\\tan ', 'tan '],\n",
        "    ['\\\\theta ', 'theta '],\n",
        "    ['\\\\theta) ', 'theta '],\n",
        "    ['\\\\theta_ ', 'theta '],\n",
        "    ['\\\\theta_0 ', 'theta không '],\n",
        "    ['\\\\theta_1 ', 'theta một '],\n",
        "    ['\\\\theta_1}(x) ', 'theta một x '],\n",
        "    ['\\\\theta_2 ', 'theta hai '],\n",
        "    ['\\\\theta_8 ', 'theta tám '],\n",
        "    ['\\\\theta} ', 'theta '],\n",
        "    ['\\\\theta}(j) ', 'theta j '],\n",
        "    ['\\\\theta}(x) ', 'theta x '],\n",
        "    ['\\\\theta}(x_1) ', 'theta x một ']\n",
        "  ]\n",
        "\n",
        "  text = row['text'].lower()\n",
        "  for item in correct:\n",
        "    text = text.replace(item[0], item[1])\n",
        "\n",
        "  text = re.sub('[^' + vocab + ']', ' ', text).strip()\n",
        "  text = ' '.join(text.split())\n",
        "\n",
        "  row['text'] = text\n",
        "  return row\n",
        "\n",
        "#Create vocab+tokenizer+processor\n",
        "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]'\n",
        "\n",
        "def remove_special_characters(batch):\n",
        "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
        "    return batch\n",
        "\n",
        "def extract_all_chars(batch):\n",
        "  all_text = \" \".join(batch[\"text\"])\n",
        "  vocab = list(set(all_text))\n",
        "  return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
        "\n",
        "def create_vocab(train_ds, test_ds, vocab_json):\n",
        "  train_ds = train_ds.map(remove_special_characters)\n",
        "  test_ds = test_ds.map(remove_special_characters)\n",
        "\n",
        "  vocab_train = train_ds.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=train_ds.column_names)\n",
        "  vocab_test = test_ds.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=test_ds.column_names)\n",
        "\n",
        "  vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))\n",
        "  vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
        "\n",
        "  vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
        "  del vocab_dict[\" \"]\n",
        "  vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
        "  vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
        "\n",
        "  with open(vocab_json, 'w') as vocab_file:\n",
        "    json.dump(vocab_dict, vocab_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HCpTxZI0XEa"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    Wav2Vec2Processor,\n",
        "    Wav2Vec2ProcessorWithLM,\n",
        "    Wav2Vec2CTCTokenizer,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2ForCTC,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "def create_tokenizer(model_path, train_ds, test_ds):\n",
        "  try:\n",
        "    tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(model_path, do_lower_case=True)\n",
        "  except:\n",
        "    vocab_json = \"./tmp_vocab.json\"\n",
        "    create_vocab(train_ds, test_ds, vocab_json)\n",
        "\n",
        "    tokenizer = Wav2Vec2CTCTokenizer(vocab_json, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
        "  return tokenizer\n",
        "\n",
        "def create_processor(model_path):\n",
        "  try:\n",
        "    processor = AutoProcessor.from_pretrained(model_path)\n",
        "  except:\n",
        "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_path)\n",
        "    tokenizer = create_tokenizer(model_path)\n",
        "    processor = Wav2Vec2Processor(feature_extractor, tokenizer)\n",
        "  return processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-tLb4dg0ZCw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "results = []\n",
        "for folder in DATA_DIR:\n",
        "  result = split_folder(folder)\n",
        "  results.extend(result)\n",
        "df = pd.DataFrame(results)\n",
        "df = df[df[\"text\"].str.find('???') == -1]\n",
        "df = df.apply(clear_text, axis=1)\n",
        "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifnXb-NS6wSe"
      },
      "outputs": [],
      "source": [
        "train_df.to_csv(os.path.join(OUTPUT_DIR, \"train_df.csv\"), index=False)\n",
        "test_df.to_csv(os.path.join(OUTPUT_DIR, \"test_df.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Nvaejqh34HZ"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "## load train dataset from pre-step\n",
        "train_ds = datasets.load_dataset('csv', data_files=os.path.join(OUTPUT_DIR, \"train_df.csv\"), keep_in_memory=True, split='train')\n",
        "test_ds = datasets.load_dataset('csv', data_files=os.path.join(OUTPUT_DIR, \"test_df.csv\"), keep_in_memory=True, split='train')\n",
        "processor = create_processor(BASE_WAV2VEC_PROCESSOR)\n",
        "def speech_file_to_array_fn(batch):\n",
        "  speech_array, sampling_rate = sf.read(batch[\"file\"])\n",
        "  batch[\"input_values\"] = processor(speech_array, sampling_rate=sampling_rate).input_values[0]\n",
        "  with processor.as_target_processor():\n",
        "      batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
        "  return batch\n",
        "train_ds = train_ds.map(speech_file_to_array_fn, remove_columns=train_ds.column_names)\n",
        "test_ds = test_ds.map(speech_file_to_array_fn, remove_columns=test_ds.column_names)\n",
        "train_ds.save_to_disk(os.path.join(OUTPUT_DIR, \"hf_datastet\", \"train\"))\n",
        "test_ds.save_to_disk(os.path.join(OUTPUT_DIR, \"hf_datastet\", \"test\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpwlUuP8Y6ak"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taMXNHpW5jot"
      },
      "source": [
        "## Distilling Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JI-g--wwMCb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klIKDUSYFDxG"
      },
      "outputs": [],
      "source": [
        "model = Wav2Vec2ForCTC.from_pretrained(BASE_WAV2VEC_MODEL)\n",
        "processor = Wav2Vec2Processor.from_pretrained(BASE_WAV2VEC_MODEL)\n",
        "print(\"Number of teacher model parameters: \", model.num_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHWQCTgQG47A"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU8UBOJjEyBi"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def deleteEncodingLayers(model, num_layers_to_keep):\n",
        "    oldModuleList = model.wav2vec2.encoder.layers\n",
        "    newModuleList = nn.ModuleList()\n",
        "\n",
        "    for i in range(0, num_layers_to_keep):\n",
        "        newModuleList.append(oldModuleList[i])\n",
        "\n",
        "    model.wav2vec2.encoder.layers = newModuleList\n",
        "    return model\n",
        "\n",
        "# distilled_wav2vec2 = deleteEncodingLayers(model, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrl1uEHmGx1o"
      },
      "outputs": [],
      "source": [
        "### Load distilled model from checkpoint\n",
        "distilled_wav2vec2 = Wav2Vec2ForCTC.from_pretrained(MY_MODEL_DIR)\n",
        "distilled_wav2vec2 = deleteEncodingLayers(distilled_wav2vec2, 6)\n",
        "print(\"Number of teacher model parameters: \", distilled_wav2vec2.num_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL-qhGvnIXXA"
      },
      "outputs": [],
      "source": [
        "distilled_wav2vec2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqj5BZqj5m6N"
      },
      "source": [
        "## Distillation Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJjZrtjxNyrq"
      },
      "outputs": [],
      "source": [
        "train_ds = datasets.load_from_disk(os.path.join(OUTPUT_DIR, \"hf_datastet\", \"train\"))\n",
        "test_ds = datasets.load_from_disk(os.path.join(OUTPUT_DIR, \"hf_datastet\", \"test\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nhWuMW1OA-z"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_ds.set_format(\"torch\")\n",
        "test_ds.set_format(\"torch\")\n",
        "train_dataloader = DataLoader(train_ds, shuffle=True, batch_size=1)\n",
        "eval_dataloader = DataLoader(test_ds, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gujbpJh4OG1E"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "optimizer = AdamW(distilled_wav2vec2.parameters(), lr=5e-5)\n",
        "\n",
        "epochs = 15\n",
        "num_training_steps = epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU0XcR1oqWgt"
      },
      "source": [
        "$$ L_{\\text{distil}} = KLLoss(\\sigma(z_{student}/T) , \\sigma(z_{teacher}/T)) * T^2 $$\n",
        "\n",
        "$$ L_{\\text{final}} = \\alpha L_{\\text{distil}} + (1 - \\alpha) L_{\\text{student}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc6_676vq4ND"
      },
      "outputs": [],
      "source": [
        "T = 4\n",
        "alpha = 0.8\n",
        "\n",
        "def compute_distil_loss(student_outputs, teacher_outputs):\n",
        "  kl_loss = torch.nn.KLDivLoss()\n",
        "  student_logits = student_outputs.logits\n",
        "  teacher_logits = teacher_outputs.logits\n",
        "  distil_loss = kl_loss(\n",
        "      F.log_softmax(student_logits/T, dim=1),\n",
        "      F.softmax(teacher_logits/T, dim=1)\n",
        "  ) * T * T\n",
        "  return distil_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC9o05DWP4w6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "teacher_model = Wav2Vec2ForCTC.from_pretrained(BASE_WAV2VEC_MODEL)\n",
        "teacher_model.eval()\n",
        "teacher_model.to(device)\n",
        "distilled_wav2vec2.to(device)\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "distilled_wav2vec2.train()\n",
        "for epoch in range(epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        student_outputs = distilled_wav2vec2(**batch)\n",
        "        teacher_outputs = teacher_model(**batch)\n",
        "\n",
        "        distil_loss = compute_distil_loss(student_outputs, teacher_outputs)\n",
        "        student_loss = student_outputs.loss\n",
        "        final_loss = alpha * distil_loss + (1. - alpha) * student_loss\n",
        "        final_loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "    distilled_wav2vec2.save_pretrained(save_directory=MY_MODEL_DIR)\n",
        "    processor.save_pretrained(save_directory=MY_MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A435cPqNcvkZ"
      },
      "source": [
        "## Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt3RzNigtpfW"
      },
      "outputs": [],
      "source": [
        "train_df = datasets.load_dataset('csv', data_files=os.path.join(OUTPUT_DIR, \"train_df.csv\"), keep_in_memory=True, split='train')\n",
        "\n",
        "text_file = os.path.join(OUTPUT_DIR, \"train_lm.txt\")\n",
        "\n",
        "# with open(text_file, 'w') as fp:\n",
        "#   for item in train_df[\"text\"]:\n",
        "#     fp.write(\"%s\\n\" % item)\n",
        "\n",
        "text_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGTht3u7ujOs"
      },
      "outputs": [],
      "source": [
        "# !bin/lmplz -o 4 </content/drive/MyDrive/Colab\\ Notebooks/Wav2vec2-ASR/data/train_lm.txt >/content/drive/MyDrive/Colab\\ Notebooks/Wav2vec2-ASR/checkpoints/lm_4.arpa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-DDiu8neRbR"
      },
      "outputs": [],
      "source": [
        "import kenlm\n",
        "from pyctcdecode import Alphabet, BeamSearchDecoderCTC, LanguageModel\n",
        "\n",
        "def get_decoder_ngram_model(tokenizer, ngram_lm_path):\n",
        "    vocab_dict = tokenizer.get_vocab()\n",
        "    sort_vocab = sorted((value, key) for (key, value) in vocab_dict.items())\n",
        "    vocab = [x[1] for x in sort_vocab][:-2]\n",
        "    vocab_list = vocab\n",
        "    # convert ctc blank character representation\n",
        "    vocab_list[tokenizer.pad_token_id] = \"\"\n",
        "    # replace special characters\n",
        "    vocab_list[tokenizer.unk_token_id] = \"\"\n",
        "    # vocab_list[tokenizer.bos_token_id] = \"\"\n",
        "    # vocab_list[tokenizer.eos_token_id] = \"\"\n",
        "    # convert space character representation\n",
        "    vocab_list[tokenizer.word_delimiter_token_id] = \" \"\n",
        "    # specify ctc blank char index, since conventially it is the last entry of the logit matrix\n",
        "    alphabet = Alphabet.build_alphabet(vocab_list, ctc_token_idx=tokenizer.pad_token_id)\n",
        "    lm_model = kenlm.Model(ngram_lm_path)\n",
        "    decoder = BeamSearchDecoderCTC(alphabet,\n",
        "                                   language_model=LanguageModel(lm_model))\n",
        "    return decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rW55amueSt-"
      },
      "outputs": [],
      "source": [
        "my_lm_file = os.path.join(MY_MODEL_DIR, \"lm_4.arpa\")\n",
        "ngram_lm_model = get_decoder_ngram_model(processor.tokenizer, my_lm_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfOPymNb5JzZ"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp8MNEko5RMB"
      },
      "source": [
        "## Teacher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMdzUz5B5I9j"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from datasets import load_metric\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "teacher_model = Wav2Vec2ForCTC.from_pretrained(BASE_WAV2VEC_MODEL)\n",
        "teacher_model.eval()\n",
        "teacher_model.to(device)\n",
        "\n",
        "def map_to_result(batch):\n",
        "    with torch.no_grad():\n",
        "        input_values = torch.tensor(batch[\"input_values\"], device=device).unsqueeze(0)\n",
        "        outputs = teacher_model(input_values)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    # batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
        "    batch[\"pred_str_lm\"] = ngram_lm_model.decode(logits[0].cpu().detach().numpy(), beam_width=500)\n",
        "    batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
        "    batch[\"outputs\"] = outputs\n",
        "    batch[\"logits\"] = logits\n",
        "\n",
        "    return batch\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "teacher_results = test_ds.map(map_to_result, remove_columns=test_ds.column_names)\n",
        "print(\"Inference time: {:.3f}\".format(time.perf_counter() - start_time))\n",
        "\n",
        "wer_metric = load_metric(\"wer\")\n",
        "# print(\"Test WER without LM: {:.3f}\".format(wer_metric.compute(predictions=teacher_results[\"pred_str\"], references=teacher_results[\"text\"])))\n",
        "print(\"Test WER with LM: {:.3f}\".format(wer_metric.compute(predictions=teacher_results[\"pred_str_lm\"], references=teacher_results[\"text\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mNUu_OwsqpR"
      },
      "outputs": [],
      "source": [
        "teacher_model.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut_E1kNr5N-N"
      },
      "outputs": [],
      "source": [
        "teacher_results[\"pred_str\"][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCRScUuNl1sS"
      },
      "outputs": [],
      "source": [
        "teacher_results[\"pred_str_lm\"][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyC5LR595O4a"
      },
      "outputs": [],
      "source": [
        "teacher_results[\"text\"][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9xitX9_5UBX"
      },
      "source": [
        "## Student"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sreX86wU4mbe"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from datasets import load_metric\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "distilled_wav2vec2.eval()\n",
        "distilled_wav2vec2.to(device)\n",
        "# processor = Wav2Vec2Processor.from_pretrained(MY_MODEL_DIR)\n",
        "\n",
        "def map_to_result(batch):\n",
        "    with torch.no_grad():\n",
        "        input_values = torch.tensor(batch[\"input_values\"], device=device).unsqueeze(0)\n",
        "        outputs = distilled_wav2vec2(input_values)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
        "    batch[\"pred_str_lm\"] = ngram_lm_model.decode(logits[0].cpu().detach().numpy(), beam_width=500)\n",
        "    batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
        "    batch[\"outputs\"] = outputs\n",
        "    batch[\"logits\"] = logits\n",
        "\n",
        "    return batch\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "student_results = test_ds.map(map_to_result, remove_columns=test_ds.column_names)\n",
        "print(\"Inference time: {:.3f}\".format(time.perf_counter() - start_time))\n",
        "\n",
        "wer_metric = load_metric(\"wer\")\n",
        "print(\"Test WER without LM: {:.3f}\".format(wer_metric.compute(predictions=student_results[\"pred_str\"], references=student_results[\"text\"])))\n",
        "print(\"Test WER with LM: {:.3f}\".format(wer_metric.compute(predictions=student_results[\"pred_str_lm\"], references=student_results[\"text\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZyGmzGxeVyM"
      },
      "outputs": [],
      "source": [
        "distilled_wav2vec2.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WZ3BDgt5NuG"
      },
      "outputs": [],
      "source": [
        "student_results[\"pred_str\"][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lld7rm3Pmk52"
      },
      "outputs": [],
      "source": [
        "student_results[\"pred_str_lm\"][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1jpGtTM5Obs"
      },
      "outputs": [],
      "source": [
        "student_results[\"text\"][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrFHUQt7mv9h"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "test_csv = os.path.join(OUTPUT_DIR, 'test_df.csv')\n",
        "audio_files = []\n",
        "with open(test_csv, newline='') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "    for row in reader:\n",
        "        audio_files.append(row[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Fiu9IqipXbL"
      },
      "outputs": [],
      "source": [
        "audio_files[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_onnx(model, onnx_model_path):\n",
        "    print(f\"Converting model to onnx\")\n",
        "\n",
        "    audio_len = 250000\n",
        "    x = torch.randn(1, audio_len, requires_grad=True)\n",
        "\n",
        "    torch.onnx.export(model,                        # model being run\n",
        "                    x,                              # model input (or a tuple for multiple inputs)\n",
        "                    onnx_model_path,                # where to save the model (can be a file or file-like object)\n",
        "                    export_params=True,             # store the trained parameter weights inside the model file\n",
        "                    opset_version=11,               # the ONNX version to export the model to\n",
        "                    do_constant_folding=True,       # whether to execute constant folding for optimization\n",
        "                    input_names = ['input'],        # the model's input names\n",
        "                    output_names = ['output'],      # the model's output names\n",
        "                    dynamic_axes={'input' : {1 : 'audio_len'},    # variable length axes\n",
        "                                'output' : {1 : 'audio_len'}})\n",
        "\n",
        "def quantize_onnx_model(onnx_model_path, quantized_model_path):\n",
        "    print(\"Starting quantization...\")\n",
        "    from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "    quantize_dynamic(onnx_model_path,\n",
        "                     quantized_model_path,\n",
        "                     weight_type=QuantType.QUInt8)\n",
        "\n",
        "    print(f\"Quantized model saved to: {quantized_model_path}\")\n",
        "\n",
        "quantize = False\n",
        "onnx_model_path = os.path.join(MY_MODEL_DIR, \"wav2vec.onnx\")\n",
        "convert_to_onnx(distilled_wav2vec2, onnx_model_path)\n",
        "if (quantize):\n",
        "    quantized_model_name = os.path.join(MY_MODEL_DIR, \"wav2vec.quant.onnx\")\n",
        "    quantize_onnx_model(onnx_model_path, quantized_model_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NCwzEsYtYvcv",
        "taMXNHpW5jot",
        "oqj5BZqj5m6N",
        "zp8MNEko5RMB"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
